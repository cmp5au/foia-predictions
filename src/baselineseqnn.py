# -*- coding: utf-8 -*-
"""BaselineSeqNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TFl9ucM64gQYWw4M3Yau_sWbzxxixw9q
"""

import numpy as np
import pandas as pd
from google.colab import drive
drive.mount('/drive')

df = pd.read_csv('/drive/MyDrive/body_multiclass_target.csv')
df['request_agency'] = pd.read_csv('/drive/MyDrive/model_features.csv')['request_agency']
df.dropna(axis=0, inplace=True)

new_labels = {'done' : 'Completed',
              'partial' : 'Completed',
              'no_docs' : 'Redacted',
              'fix' : 'Rejected',
              'rejected' : 'Rejected'}
df.loc[:, 'target'] = df.target.apply(lambda x: new_labels[x])
statuses = df['target'].values

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
train, test = train_test_split(np.arange(len(df)), test_size=0.15,
                               stratify=df.target.values,
                               random_state=42)
tfidf.fit(df.body.values[train])
X = np.concatenate((tfidf.transform(df.body.values).toarray(),
                    df.request_agency.values.reshape(-1, 1)), axis=1)
y = df.target.values
X.shape, y.shape

# tfidf_mat.shape, y.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation

num_coef = df.values.shape[1]
model = Sequential()

model.add(Dense(units=64,
                input_shape=(5001,)))
model.add(Dropout(0.5))
model.add(Activation('sigmoid'))
model.add(Dense(units=64))
model.add(Dropout(0.5))
model.add(Activation('sigmoid'))
model.add(Dense(3))
model.add(Activation('softmax'))
model.summary()
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['acc'])

# d = {}
# for i, status in enumerate(set(statuses)):
#   d[status] = i

# y = np.array([d[status] for status in statuses])

y = pd.get_dummies(statuses).values

model.fit(X[train], y[train], batch_size=32, epochs=5, verbose=1, validation_split=0.2)

# model 2, reducing overfitting by reducing number of neurons in dense layers

model2 = Sequential()

model2.add(Dense(units=20,
                input_shape=(5001,)))
model2.add(Dropout(0.2))
model2.add(Activation('relu'))
model2.add(Dense(units=20))
model2.add(Dropout(0.2))
model2.add(Activation('relu'))
model2.add(Dense(3))
model2.add(Activation('softmax'))
model2.summary()
model2.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['acc'])

model2.fit(X[train], y[train], batch_size=32, epochs=10, verbose=1, validation_split=0.2)

# model 3, reducing overfitting by reducing number of dense layers

model3 = Sequential()

model3.add(Dense(units=64,
                input_shape=(5001,)))
model3.add(Dropout(0.2))
model3.add(Activation('relu'))
model3.add(Dense(3))
model3.add(Activation('softmax'))
model3.summary()
model3.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['acc'])

model3.fit(X[train], y[train], batch_size=32, epochs=10, verbose=1, validation_split=0.2)

# model 4, reducing overfitting by increasing dropout rate

model4 = Sequential()

model4.add(Dense(units=64,
                input_shape=(5001,)))
model4.add(Dropout(0.4))
model4.add(Activation('relu'))
model4.add(Dense(units=64))
model4.add(Dropout(0.4))
model4.add(Activation('relu'))
model4.add(Dense(3))
model4.add(Activation('softmax'))
model4.summary()
model4.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['acc'])

model4.fit(X[train], y[train], batch_size=32, epochs=15, verbose=1, validation_split=0.2)

import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

idxs = np.random.choice(range(X[test].shape[0]), size=1000, replace=False)

colors = np.array(['g' if y[0] else 'k' if y[1] else 'r'
                    for y in y[test]])

tsne = TSNE(n_components=2)
tsne_3d = TSNE(n_components=3)
proba_preds = model4.predict_proba(X[test])
preds = model4.predict(X[test])

scatter = tsne.fit_transform(proba_preds[idxs])

fig, ax = plt.subplots()

xs = scatter[:, 0]
ys = scatter[:, 1]

ax.scatter(xs, ys, c=colors[idxs])
ax.set_title("Sequential Neural Network Classifier visualized with t-SNE")

scatter_3d = tsne_3d.fit_transform(proba_preds[idxs])

xs_3d = scatter_3d[:, 0]
ys_3d = scatter_3d[:, 1]
zs_3d = scatter_3d[:, 2]

ax = plt.figure().gca(projection='3d')

ax.scatter(xs_3d, ys_3d, zs_3d, c=colors[idxs])
ax.set_title("Sequential Neural Network Classifier visualized with t-SNE")

y[test][0]

# model 5, reducing overfitting by increasing dropout rate
#          and reducing number of neurons per layer

model5 = Sequential()

model5.add(Dense(units=20,
                input_shape=(5001,)))
model5.add(Dropout(0.4))
model5.add(Activation('relu'))
model5.add(Dense(units=20))
model5.add(Dropout(0.4))
model5.add(Activation('relu'))
model5.add(Dense(3))
model5.add(Activation('softmax'))
model5.summary()
model5.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['acc'])

model5.fit(X[train], y[train], batch_size=32, epochs=10, verbose=1, validation_split=0.2)

from tensorflow.keras.preprocessing import text, sequence 

max_len = 512
max_words = 20000
tokenizer = text.Tokenizer(num_words = max_words)
# create the vocabulary by fitting on x_train text
tokenizer.fit_on_texts(df.body.values[train])
# generate the sequence of tokens
xtrain_seq = tokenizer.texts_to_sequences(df.body.values[train])
xtest_seq = tokenizer.texts_to_sequences(df.body.values[test])

# pad the sequences
xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)
xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)
word_index = tokenizer.word_index

print('text example:', df.body.values[train][0])
print('sequence of indices(before padding):', xtrain_seq[0])
print('sequence of indices(after padding):', xtrain_pad[0])

embedding_vectors = {}
with open('/drive/MyDrive/glove.6B.100d.txt','r',encoding='utf-8') as file:
    for row in file:
        values = row.split(' ')
        word = values[0]
        weights = np.asarray([float(val) for val in values[1:]])
        embedding_vectors[word] = weights

#initialize the embedding_matrix with zeros
emb_dim = 100
if max_words is not None: 
    vocab_len = max_words 
else:
    vocab_len = len(word_index)+1
embedding_matrix = np.zeros((vocab_len, emb_dim))
oov_count = 0
oov_words = []
for word, idx in word_index.items():
    if idx < vocab_len:
        embedding_vector = embedding_vectors.get(word)
        if embedding_vector is not None:
            embedding_matrix[idx] = embedding_vector
        else:
            oov_count += 1 
            oov_words.append(word)

from tensorflow.keras.layers import LSTM, Embedding

lstm_model = Sequential()
lstm_model.add(Embedding(vocab_len, emb_dim, trainable = False, weights=[embedding_matrix]))
lstm_model.add(LSTM(10, return_sequences=False))
lstm_model.add(Dropout(0.2))
lstm_model.add(Dense(3, activation='softmax'))
lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(lstm_model.summary())

batch_size = 256
epochs  = 100
history = lstm_model.fit(xtrain_pad, np.asarray(y[train]), validation_data=(xtest_pad, np.asarray(y[test])), batch_size = batch_size, epochs = epochs)

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

idxs = np.random.choice(range(len(xtest_pad)), size=1000, replace=False)

colors = np.array(['g' if y[0] else 'k' if y[1] else 'r'
                    for y in y[test]])

tsne = TSNE(n_components=2)
tsne_3d = TSNE(n_components=3)
proba_preds = lstm_model.predict_proba(xtest_pad[idxs])

# preds = lstm_model.predict(X[test])

scatter = tsne.fit_transform(proba_preds)

fig, ax = plt.subplots()

xs = scatter[:, 0]
ys = scatter[:, 1]

ax.scatter(xs, ys, c=colors[idxs])
ax.set_title("LSTM Classifier visualized with t-SNE")

scatter_3d = tsne_3d.fit_transform(proba_preds)

xs_3d = scatter_3d[:, 0]
ys_3d = scatter_3d[:, 1]
zs_3d = scatter_3d[:, 2]

ax = plt.figure().gca(projection='3d')

ax.scatter(xs_3d, ys_3d, zs_3d, c=colors[idxs])
ax.set_title("LSTM Classifier visualized with t-SNE")

from sklearn.metrics import accuracy_score, f1_score

preds = np.argmax(lstm_model.predict(xtest_pad), axis=1)
accuracy_score(np.argmax(y[test], axis=1), preds), f1_score(np.argmax(y[test], axis=1), preds, average='weighted')



